/* Copyright 2021 The TensorFlow Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

syntax = "proto2";

package tflite.task.vision;

import "tensorflow/lite/acceleration/configuration/configuration.proto";
import "tensorflow_lite_support/cc/task/core/proto/external_file.proto";

// Options for setting up an ImageEmbedder.
// Next Id: 10.
message ImageEmbedderOptions {
  // The external model file, as a single standalone TFLite, optionally packed
  // with TFLite Model Metadata [1]. Those are mandatory only if the model input
  // is of float type (kTfLiteFloat32), which requires `NormalizationOptions` to
  // be set on the input tensor metadata.
  //
  // [1]: https://www.tensorflow.org/lite/convert/metadata
  optional core.ExternalFile model_file_with_metadata = 9;

  // Whether to normalize the returned feature vector with L2 norm. Use this
  // option only if the model does not already contain a native L2_NORMALIZATION
  // TF Lite Op. In most cases, this is already the case and L2 norm is thus
  // achieved through TF Lite inference.
  optional bool l2_normalize = 6;

  // Whether the returned embedding should be quantized to bytes via scalar
  // quantization. Embeddings are implicitly assumed to be unit-norm and
  // therefore any dimension is guaranteed to have a value in [-1.0, 1.0]. Use
  // the l2_normalize option if this is not the case.
  optional bool quantize = 2;

  // The number of threads allowed for model inference. This value is used in
  // building the TF Lite interpreter.
  optional int32 num_threads = 7 [default = 1];

  // Advanced settings specifying how to accelerate the model inference using
  // dedicated delegates. Supported delegate type includes:
  // NONE, NNAPI, GPU, HEXAGON, XNNPACK, EDGETPU (Google internal),
  // and EDGETPU_CORAL.
  //
  // IMPORTANT: in order to use a delegate, the appropriate delegate plugin
  // needs to be linked at build time. See comment above the "image_embedder"
  // target at:
  // https://github.com/tensorflow/tflite-support/blob/master/tensorflow_lite_support/cc/task/vision/BUILD
  //
  // See settings definition at:
  // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/acceleration/configuration/configuration.proto
  optional tflite.proto.ComputeSettings compute_settings = 8;

  // Reserved tags.
  reserved 1, 3, 4, 5;
}
